{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MounishaChalasani/MounishaChalasani-INFO5731_Assignment_One_fall2022/blob/main/INFO5731_Assignment_Two_fall2022_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 100 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "bbccfb84-6aea-4761-a4dd-fa5cc8905a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.5.0-py3-none-any.whl (995 kB)\n",
            "\u001b[K     |████████████████████████████████| 995 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[K     |████████████████████████████████| 384 kB 55.3 MB/s \n",
            "\u001b[?25hCollecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.7/dist-packages (from selenium) (2022.9.24)\n",
            "Collecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (22.1.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, urllib3, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.0.0rc9 h11-0.14.0 outcome-1.2.0 selenium-4.5.0 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.12 wsproto-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,183 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,550 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,005 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,328 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,436 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,223 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,164 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,109 kB]\n",
            "Fetched 16.3 MB in 4s (3,887 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 91.7 MB of archives.\n",
            "After this operation, 309 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 105.0.5195.102-0ubuntu0.18.04.1 [1,156 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 105.0.5195.102-0ubuntu0.18.04.1 [80.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 105.0.5195.102-0ubuntu0.18.04.1 [5,097 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 105.0.5195.102-0ubuntu0.18.04.1 [5,320 kB]\n",
            "Fetched 91.7 MB in 4s (26.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_105.0.5195.102-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_105.0.5195.102-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (105.0.5195.102-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import requests, openpyxl\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import urllib.request as req\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
        "from selenium import webdriver \n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "WPYAE06sC6q-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "titles = [] \n",
        "reviews= []\n",
        "url = 'https://www.imdb.com/title/tt7286456/reviews'\n",
        "connect = webdriver.Chrome('chromedriver',options=options)\n",
        "connect.get(url)\n",
        "for i in range(40):\n",
        "  connect.find_element(By.CLASS_NAME, \"ipl-load-more__button\").click()\n",
        "  time.sleep(5)\n",
        "  All_Title = connect.find_elements(By.CLASS_NAME, \"title\")\n",
        "  All_Reviews = connect.find_elements(By.CLASS_NAME, \"text\")\n",
        "\n",
        "for e, r in zip(All_Title, All_Reviews):\n",
        "      titles.append((e.text).replace('\\n',''))\n",
        "      reviews.append(r.text)\n",
        "\n",
        "data = pd.DataFrame(list(zip(titles, reviews)), columns =['Title', 'Review'])\n",
        "\n",
        "print(\"Length is :\",len(data))\n",
        "print(data)"
      ],
      "metadata": {
        "id": "sPRyUK6MDCvu",
        "outputId": "508d4dc0-633d-4a70-ed74-06ef34d472b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length is : 1022\n",
            "                                                  Title  \\\n",
            "0                              Perfect in every aspect.   \n",
            "1     A psychological study, rather than a superhero...   \n",
            "2                       Went for a second time to watch   \n",
            "3              JUST AMAZING. How does this movie exist.   \n",
            "4     Outstanding movie with a haunting performance ...   \n",
            "...                                                 ...   \n",
            "1017              I have serious issues with this movie   \n",
            "1018                            Gives me the right vibe   \n",
            "1019                                                Wow   \n",
            "1020        One of the best movies I have seen in years   \n",
            "1021                                           Stunning   \n",
            "\n",
            "                                                 Review  \n",
            "0     Truly a masterpiece, The Best Hollywood film o...  \n",
            "1     I have seen Joker yesterday at Venice an early...  \n",
            "2     I get why some people hate this . It's because...  \n",
            "3     Let me start off by saying if Joaquin Phoneix ...  \n",
            "4     Every once in a while a movie comes, that trul...  \n",
            "...                                                 ...  \n",
            "1017  I heard a lot of praise for the Joker movie ba...  \n",
            "1018                                                     \n",
            "1019  This movie is a masterpiece, well directed, th...  \n",
            "1020  I have to admit that I don't really care about...  \n",
            "1021  Wow, amazing acting from Phoenix as usual. Tax...  \n",
            "\n",
            "[1022 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "9f4b76f3-f7c9-42fb-86af-366bc9e0759d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "metadata": {
        "id": "qr9LSylnRmwF",
        "outputId": "b06e8d41-ae05-4066-a072-6d68482852e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=data\n",
        "df=df[df[\"Review\"]!= \"\"]"
      ],
      "metadata": {
        "id": "U33LjzuwRos0",
        "outputId": "cd78337b-336f-447b-ffba-4213b8aa040c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  Title  \\\n",
            "0                              Perfect in every aspect.   \n",
            "1     A psychological study, rather than a superhero...   \n",
            "2                       Went for a second time to watch   \n",
            "3              JUST AMAZING. How does this movie exist.   \n",
            "4     Outstanding movie with a haunting performance ...   \n",
            "...                                                 ...   \n",
            "1016        Do not believe the mainstream paid critics!   \n",
            "1017              I have serious issues with this movie   \n",
            "1019                                                Wow   \n",
            "1020        One of the best movies I have seen in years   \n",
            "1021                                           Stunning   \n",
            "\n",
            "                                                 Review  \n",
            "0     Truly a masterpiece, The Best Hollywood film o...  \n",
            "1     I have seen Joker yesterday at Venice an early...  \n",
            "2     I get why some people hate this . It's because...  \n",
            "3     Let me start off by saying if Joaquin Phoneix ...  \n",
            "4     Every once in a while a movie comes, that trul...  \n",
            "...                                                 ...  \n",
            "1016  I'd have given it an 8 or 9. But decided on 10...  \n",
            "1017  I heard a lot of praise for the Joker movie ba...  \n",
            "1019  This movie is a masterpiece, well directed, th...  \n",
            "1020  I have to admit that I don't really care about...  \n",
            "1021  Wow, amazing acting from Phoenix as usual. Tax...  \n",
            "\n",
            "[878 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "stopword = stopwords.words('english')\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean(data):\n",
        "    data.loc[:,[\"clean_txt\"]]=df[\"Review\"].apply(lambda ele: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", ele))\n",
        "    data.loc[:,[\"clean_txt\"]]=df[\"clean_txt\"].apply(lambda ele: re.sub(r\"\\d+\", \"\", ele))\n",
        "    data.loc[:,[\"clean_txt\"]]=df[\"clean_txt\"].apply(lambda string: ' '.join([w for w in string.split() if w not in (stopword_list)]))\n",
        "    data.loc[:,[\"clean_txt\"]]=df[\"clean_txt\"].str.lower()\n",
        "    data.loc[:,[\"tokens\"]] = df[\"clean_txt\"].apply(lambda x: word_tokenize(x))\n",
        "    def word_stemmer(text):\n",
        "      stem_text = [snow_stemmer.stem(i) for i in text]\n",
        "      return stem_text\n",
        "    data.loc[:,['text_stem']] = df[\"tokens\"].apply(lambda x: word_stemmer(x))\n",
        "    def word_lemmatizer(text):\n",
        "      lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
        "      return lem_text\n",
        "    data.loc[:,['text_lem']] = df[\"tokens\"].apply(lambda x: word_lemmatizer(x))\n",
        "    return data\n",
        "c = clean(data)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "8OMo6ibzRq3z",
        "outputId": "6e9a47a7-36fe-4358-f853-e7ce80cc0c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  Title  \\\n",
            "0                              Perfect in every aspect.   \n",
            "1     A psychological study, rather than a superhero...   \n",
            "2                       Went for a second time to watch   \n",
            "3              JUST AMAZING. How does this movie exist.   \n",
            "4     Outstanding movie with a haunting performance ...   \n",
            "...                                                 ...   \n",
            "1017              I have serious issues with this movie   \n",
            "1018                            Gives me the right vibe   \n",
            "1019                                                Wow   \n",
            "1020        One of the best movies I have seen in years   \n",
            "1021                                           Stunning   \n",
            "\n",
            "                                                 Review  \\\n",
            "0     Truly a masterpiece, The Best Hollywood film o...   \n",
            "1     I have seen Joker yesterday at Venice an early...   \n",
            "2     I get why some people hate this . It's because...   \n",
            "3     Let me start off by saying if Joaquin Phoneix ...   \n",
            "4     Every once in a while a movie comes, that trul...   \n",
            "...                                                 ...   \n",
            "1017  I heard a lot of praise for the Joker movie ba...   \n",
            "1018                                                      \n",
            "1019  This movie is a masterpiece, well directed, th...   \n",
            "1020  I have to admit that I don't really care about...   \n",
            "1021  Wow, amazing acting from Phoenix as usual. Tax...   \n",
            "\n",
            "                                              clean_txt  \\\n",
            "0     truly masterpiece the best hollywood film one ...   \n",
            "1     i seen joker yesterday venice early illfated s...   \n",
            "2     i get people hate its political message people...   \n",
            "3     let start saying joaquin phoneix doesnt get os...   \n",
            "4     every movie comes truly makes impact joaquins ...   \n",
            "...                                                 ...   \n",
            "1017  i heard lot praise joker movie back theatres n...   \n",
            "1018                                                NaN   \n",
            "1019  this movie masterpiece well directed cinematog...   \n",
            "1020  i admit i dont really care superhero movies bu...   \n",
            "1021  wow amazing acting phoenix usual taxi driver s...   \n",
            "\n",
            "                                                 tokens  \\\n",
            "0     [truly, masterpiece, the, best, hollywood, fil...   \n",
            "1     [i, seen, joker, yesterday, venice, early, ill...   \n",
            "2     [i, get, people, hate, its, political, message...   \n",
            "3     [let, start, saying, joaquin, phoneix, doesnt,...   \n",
            "4     [every, movie, comes, truly, makes, impact, jo...   \n",
            "...                                                 ...   \n",
            "1017  [i, heard, lot, praise, joker, movie, back, th...   \n",
            "1018                                                NaN   \n",
            "1019  [this, movie, masterpiece, well, directed, cin...   \n",
            "1020  [i, admit, i, dont, really, care, superhero, m...   \n",
            "1021  [wow, amazing, acting, phoenix, usual, taxi, d...   \n",
            "\n",
            "                                              text_stem  \\\n",
            "0     [truli, masterpiec, the, best, hollywood, film...   \n",
            "1     [i, seen, joker, yesterday, venic, earli, illf...   \n",
            "2     [i, get, peopl, hate, it, polit, messag, peopl...   \n",
            "3     [let, start, say, joaquin, phoneix, doesnt, ge...   \n",
            "4     [everi, movi, come, truli, make, impact, joaqu...   \n",
            "...                                                 ...   \n",
            "1017  [i, heard, lot, prais, joker, movi, back, thea...   \n",
            "1018                                                NaN   \n",
            "1019  [this, movi, masterpiec, well, direct, cinemat...   \n",
            "1020  [i, admit, i, dont, realli, care, superhero, m...   \n",
            "1021  [wow, amaz, act, phoenix, usual, taxi, driver,...   \n",
            "\n",
            "                                               text_lem  \n",
            "0     [truly, masterpiece, the, best, hollywood, fil...  \n",
            "1     [i, seen, joker, yesterday, venice, early, ill...  \n",
            "2     [i, get, people, hate, it, political, message,...  \n",
            "3     [let, start, saying, joaquin, phoneix, doesnt,...  \n",
            "4     [every, movie, come, truly, make, impact, joaq...  \n",
            "...                                                 ...  \n",
            "1017  [i, heard, lot, praise, joker, movie, back, th...  \n",
            "1018                                                NaN  \n",
            "1019  [this, movie, masterpiece, well, directed, cin...  \n",
            "1020  [i, admit, i, dont, really, care, superhero, m...  \n",
            "1021  [wow, amazing, acting, phoenix, usual, taxi, d...  \n",
            "\n",
            "[1022 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "outputId": "0e2eacd5-8bf1-4b2e-fea1-4350f400929d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.4.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.12.1+cu113)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 8.9 MB/s \n",
            "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Downloading transformers-4.23.0-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.17.3)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (7.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.9.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (8.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.9->benepar) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=2.0.9->benepar) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0.9->benepar) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 54.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.9->benepar) (0.7.8)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (5.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.0.9->benepar) (2.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->benepar) (1.15.0)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37647 sha256=fd7522d49c76d383f4d9fbf2c73aafdff56d5ed9ee26046fbc93ec850aca7213\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/6f/a3/4d27ce92766bdedd2cbbbedb8857fb7a53534331191cda4994\n",
            "Successfully built benepar\n",
            "Installing collected packages: urllib3, tokenizers, huggingface-hub, transformers, torch-struct, sentencepiece, benepar\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.12\n",
            "    Uninstalling urllib3-1.26.12:\n",
            "      Successfully uninstalled urllib3-1.26.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.5.0 requires urllib3[socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\u001b[0m\n",
            "Successfully installed benepar-0.2.0 huggingface-hub-0.10.0 sentencepiece-0.1.97 tokenizers-0.13.1 torch-struct-0.5 transformers-4.23.0 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading benepar_en2: Package 'benepar_en2' not found\n",
            "[nltk_data]     in index\n",
            "/usr/local/lib/python3.7/dist-packages/benepar/spacy_plugin.py:12: FutureWarning: BeneparComponent and NonConstituentException have been moved to the benepar module. Use `from benepar import BeneparComponent, NonConstituentException` instead of benepar.spacy_plugin. The benepar.spacy_plugin namespace is deprecated and will be removed in a future version.\n",
            "  FutureWarning,\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "%tensorflow_version 2.x\n",
        "!pip install benepar\n",
        "import benepar\n",
        "benepar.download('benepar_en2')\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "benepar.download('benepar_en3')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def struc(text):\n",
        "  for i in text:\n",
        "    for token in nlp(i):\n",
        "      return [token.text, '-',token.pos_,'-',token.tag_]\n",
        "data['pos']=data['tokens'].apply(lambda x: struc(x))\n",
        "print(data['pos'].head())\n",
        "print(\"length:\",len(data['pos']))"
      ],
      "metadata": {
        "id": "gLs0pNAsRv40",
        "outputId": "df7fa42a-15a0-4213-e2e9-eeb25e69d7b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     [truly, -, ADV, -, RB]\n",
            "1       [i, -, PRON, -, PRP]\n",
            "2       [i, -, PRON, -, PRP]\n",
            "3      [let, -, VERB, -, VB]\n",
            "4    [every, -, PRON, -, DT]\n",
            "Name: pos, dtype: object\n",
            "length: 878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging, dependency parsing,constituency parsing will help us to undesrtand the data better. **POS tags** tells about the parts of speech in the sentence of words. **Dependency Parsing** will tell us about the dependencies between the words in a sentence. **Constituency Parsing** will create trees to represent the sentence according to the grammar."
      ],
      "metadata": {
        "id": "BzdukUbC9C77"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8g0Qw4g-G_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}